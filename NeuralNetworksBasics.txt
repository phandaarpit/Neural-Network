Notes from Principles of Soft Computing:

Neural Network:
  Supervised learning:
      Inputs plus the target values are fed to the NN.
      Errors are found between the output values and desired values and NN learns from this.

  Unsupervised Learning:
      No target values are specified
      NN uses the clustering to cluster or just form the pattern between the related inputs.

Why use non linear activation function?
  Well if it had been a linear activation function in a multilayer network then it would behave as a single layer
  network only.

    Why sigmoidal function is used in Backpropagation?
      it is used to reduce the overhead of finding the derivative at a point which is heavily used in backpropagation.

Back-propagation Neural Network:
  Important Concepts:
      Weight Update Algorithm: Basically gradient descent to reach to least or zero error rate;
                               error rate is propagated backwards to hidden layers [df(x)/dx]
      Three stages of BPN:
        Feed forwarding the inputs
        Find error between the target values and desired output, Backpropagate the error
        Update the weights

      Activation function is any function which follows the following properties:
        increasing monotonically
        differentiable
          Sigmoid function follows both properties and is also used alot in BPNs.

    Training Algorithm:
      1. Each input neuron in input layer of net receives input and fed forward to
         next layer.
      2. Each neuron in hidden layer sums up (wi)(xi), where wi is the weight and xi is the
         input to the neuron.
         + the bias
      3. then apply the activation function on the summation
      4. similar for the output layer neurons

    Back-propagation of error:
      1. calculate between output neuron output and target value
      2. propagate the error backward into hidden layer, taking momentum and learning rate into consideration.


  Learning rate:
    basically is the rate of convergence, if the value is too low it will lead to slow learning but if the value is high it
    can lead to overshooting and since it will skip the local minima it will keep oscillating about the local minima

  Momentum:
    since high learning rate leads to overshoot and oscillations thats where momentum comes into play, it helps in convergence with
    high learning rate.

  Drawbacks :
    Since its gradient descent approach in nutshell therefore there is a tendency to get stuck in the local minima, and not the global
    minima which can lead to not so satisfactory results.
